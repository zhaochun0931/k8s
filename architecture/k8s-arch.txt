https://kubernetes.io/docs/concepts/overview/components/




master(control plane) components 
|--- kube-apiserver
|--- etcd
|--- kube-scheduler
|--- kube-controller-manager
|--- cloud-controller-manager





node components 
|--- kubelet
|--- kube-proxy
|--- Container runtime






/etc/kubernetes
/etc/kubernetes/pki





# ls -al /etc/kubernetes/
total 44
drwxrwxr-x   4 root root 4096 Oct 18 14:08 .
drwxr-xr-x 101 root root 4096 Oct 18 14:07 ..
-rw-------   1 root root 5640 Oct 18 14:08 admin.conf
-rw-------   1 root root 5672 Oct 18 14:08 controller-manager.conf
-rw-------   1 root root 1960 Oct 18 14:08 kubelet.conf
drwxrwxr-x   2 root root 4096 Oct 18 14:08 manifests
drwxr-xr-x   3 root root 4096 Oct 18 14:08 pki
-rw-------   1 root root 5624 Oct 18 14:08 scheduler.conf
#






kubelet

The kubelet is the primary "node agent" that runs on each node. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. 
The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.


Each worker node runs a component called kubelet. The kubelet is responsible for managing the containers on that node.
The kubelet communicates with the Kubernetes API server to report the status of the node and its containers, and to receive instructions on what to run.







Pod controller
--- ReplicationController
--- ReplicaSet
--- Deployment
--- StatefulSet
--- DaemonSet
--- Job
--- CronJob





--- stateful application
--- stateless application

Stateful applications maintain some form of state or data across interactions. They often require persistent storage. Deployed using StatefulSets

Stateless applications do not retain any information about their previous interactions. Each request is treated independently.  Typically deployed using Deployments or ReplicaSets.








Kubernetes Services
|--- ClusterIP
|--- NodePort
|--- LoadBalancer
|--- ExternalName


The ClusterIP Service is good for internal-only applications that support other workloads within the cluster. ClusterIP is used by default if you don't explicitly specify a type for a Service.


NodePort creates a port mapping on the underlying node that allows the application to be accessed directly with the node IP address and port. This means the Service is accessible from outside the cluster, by requesting <NodeIP>:<NodePort>.









Services operate at Layer 4 (TCP/UDP), while Ingress works at Layer 7 (HTTP/S).







/etc/resolv.conf







every Service gets its own stable IP address, DNS name, and port.

Service uses labels to dynamically associate with a set of Pods.


Pods and Services are loosely coupled via labels and label selectors.


For a Service to match a set of Pods, and therefore provide stable networking and load-balance, it only needs to match some of the Pods labels.





Each Service that is created automatically gets an associated Endpoint object. This Endpoint object is a dynamic list of all of the Pods that match the Service’s label selector.



When we create a Service object, Kubernetes automatically creates an internal DNS mapping for it. 
This maps the name of the Service to a VIP. For example, if you create a Service called “hellcat-svc”, Pods in the cluster will be able to resolve “hellcat-svc” to the Service’s VIP. 
Therefore, as long as you know the name of a Service, you will be able to connect to it by name.



kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0 --replicas=3
kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:2.0 --replicas=3

kubectl expose deployment hello-server --name=hello-server-svc-np --type NodePort

kubectl expose deployment hello-server --name=hello-server-svc-cip --port=80




