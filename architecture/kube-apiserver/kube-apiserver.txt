At the highest level, Kubernetes is an orchestrator of containerized apps. Ideally microservice apps. Microservice app is just a fancy name for an application that’s made up of lots of small and independent parts - we sometimes call these small parts services. 
These small independent services work together to create a meaningful/useful app.


In the real world, a football (soccer) team is made up of individuals. No two are the same, and each has a different role to play in the team. 
Some defend, some attack, some are great at passing, some are great at shooting…. Along comes the coach, and he or she gives everyone a position and organizes them into a team with a plan.






master:

1. kube-apiserver
2. etcd database
3. kube-controller-manager
4. kube-scheduler
5. cloud-controller-manager






kube-apiserver

The API Server (apiserver) is the frontend into the Kubernetes control plane. You can think of the API server as the brains of the cluster. By default, it exposes a RESTful endpoint on port 443

kube-apiserver 支持同时提供 https（默认监听在 6443 端口）和 http API（默认监听在 127.0.0.1 的 8080 端口），其中 http API 是非安全接口，不做任何认证授权机制，不建议生产环境启用。两个接口提供的 REST API 格式相同


在实际使用中，通常通过 kubectl 来访问 apiserver，也可以通过 Kubernetes 各个语言的 client 库来访问 apiserver。在使用 kubectl 时，打开调试日志也可以看到每个 API 调用的格式，比如
kubectl --v=8 get pods










The cluster store
If the API Server is the brains of the cluster, the cluster store is its memory, The cluster store is based on etcd, the popular distributed, consistent and watchable key-value store.


kube-controller-manager
They tend to sit in loops and watch for changes – the aim of the game is to make sure the current state of the cluster matches the desired state (more on this shortly).











cloud-controller-manager







node:

1. kubelet
2. kube-proxy
3. Container runtime



每台机器上都运行一个 kube-proxy 服务，它监听 API server 中 service 和 endpoint 的变化情况，并通过 iptables 等来为服务配置负载均衡（仅支持 TCP 和 UDP）.
kube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行.













每个Node节点上都运行一个 Kubelet 服务进程，默认监听 10250 端口，接收并执行 Master 发来的指令，管理 Pod 及 Pod 中的容器。每个 Kubelet 进程会在 API Server 上注册所在Node节点的信息，定期向 Master 节点汇报该节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源.

Kubelet 监听 etcd，所有针对 Pod 的操作都将会被 Kubelet 监听到。如果发现有新的绑定到本节点的 Pod，则按照 Pod 清单的要求创建该 Pod。

如果发现本地的 Pod 被修改，则 Kubelet 会做出相应的修改，比如删除 Pod 中某个容器时，则通过 Docker Client 删除该容器。 如果发现删除本节点的 Pod，则删除相应的 Pod，并通过 Docker Client 删除 Pod 中的容器。

Kubelet 读取监听到的信息，如果是创建和修改 Pod 任务，则执行如下处理：

为该 Pod 创建一个数据目录；
从 API Server 读取该 Pod 清单；
为该 Pod 挂载外部卷；
下载 Pod 用到的 Secret；
检查已经在节点上运行的 Pod，如果该 Pod 没有容器或 Pause 容器没有启动，则先停止 Pod 里所有容器的进程。如果在 Pod 中有需要删除的容器，则删除这些容器；
用 “kubernetes/pause” 镜像为每个 Pod 创建一个容器。Pause 容器用于接管 Pod 中所有其他容器的网络。每创建一个新的 Pod，Kubelet 都会先创建一个 Pause 容器，然后创建其他容器。
为 Pod 中的每个容器做如下处理：
为容器计算一个 hash 值，然后用容器的名字去 Docker 查询对应容器的 hash 值。若查找到容器，且两者 hash 值不同，则停止 Docker 中容器的进程，并停止与之关联的 Pause 容器的进程；若两者相同，则不做任何处理；
如果容器被终止了，且容器没有指定的 restartPolicy，则不做任何处理；
调用 Docker Client 下载容器镜像，调用 Docker Client 运行容器。





In Kubernetes, the two concepts work like this:
1. We declare the desired state of our application (microservice) in a manifest file
2. We POST it the API server
3. Kubernetes stores this in the cluster store as the application’s desired state
4. Kubernetes deploys the application on the cluster
5. Kubernetes implements watch loops to make sure the cluster doesn’t vary from desired state
















Port 6443 is used by default for the Kubernetes API server's HTTPS service


kubectl cluster-info





# kubectl cluster-info
Kubernetes control plane is running at https://10.1.0.4:6443
CoreDNS is running at https://10.1.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
#







# kubectl get nodes
NAME   STATUS   ROLES           AGE     VERSION
gf1    Ready    control-plane   3d15h   v1.32.3
#

cat ~/.kube/config | egrep -i 'certificate-authority-data' | awk -F: '{print $2}' | tr -d ' ' | base64 -d > myca.pem

cat ~/.kube/config | egrep -i 'client-certificate-data' | awk -F: '{print $2}' | tr -d ' ' | base64 -d > mycert.pem

cat ~/.kube/config | egrep -i 'client-key-data' | awk -F: '{print $2}' | tr -d ' ' | base64 -d > mykey.pem

curl --cacert myca.pem --cert mycert.pem --key mykey.pem https://gf1:6443











$ docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS        PORTS                       NAMES
5ff2a375f4d6   kindest/node:v1.34.0   "/usr/local/bin/entr…"   31 hours ago   Up 11 hours                               my-cluster-worker3
f85179a8a4fd   kindest/node:v1.34.0   "/usr/local/bin/entr…"   31 hours ago   Up 11 hours                               my-cluster-worker
8ed41ece1450   kindest/node:v1.34.0   "/usr/local/bin/entr…"   31 hours ago   Up 11 hours   127.0.0.1:62478->6443/tcp   my-cluster-control-plane
a15639704542   kindest/node:v1.34.0   "/usr/local/bin/entr…"   31 hours ago   Up 11 hours                               my-cluster-worker2
$


docker exec -it my-cluster-control-plane bash





# ps -auxww
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.0  22268 12780 ?        Ss   04:38   0:01 /sbin/init
root          91  0.0  0.0  25120 10388 ?        Ss   04:38   0:00 /lib/systemd/systemd-journald
root         107  0.7  0.4 3048508 70816 ?       Ssl  04:38   4:10 /usr/local/bin/containerd
root         186  3.1  0.5 2617196 97472 ?       Ssl  04:38  17:40 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --node-ip=172.26.0.3 --node-labels= --pod-infra-container-image=registry.k8s.io/pause:3.10.1 --provider-id=kind://docker/my-cluster/my-cluster-control-plane --runtime-cgroups=/system.slice/containerd.service
root         332  0.0  0.0 1233200 12604 ?       Sl   04:38   0:07 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 2224fe28d01e71757010d6d98ee0f8a53baa2a6c37b18eed0c9ad59ea8f9f0f2 -address /run/containerd/containerd.sock
root         351  0.0  0.0 1233456 13192 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 8c40a28b4505f2e53f8c7aade7072c082d84431f787496d145835f67bb48d7d8 -address /run/containerd/containerd.sock
root         379  0.0  0.0 1233456 15232 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id b0969d48ec282fbd3f7ff846797761bf7d038eda14c8d6adec28bdfe1842210d -address /run/containerd/containerd.sock
root         394  0.0  0.0 1233456 14996 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 5a746376a3a77af8d27dc5310b59182e212b88a3b9554fc825fcf4762f51508a -address /run/containerd/containerd.sock
65535        447  0.0  0.0    796   488 ?        Ss   04:38   0:00 /pause
65535        454  0.0  0.0    796   492 ?        Ss   04:38   0:00 /pause
65535        456  0.0  0.0    796   496 ?        Ss   04:38   0:00 /pause
65535        466  0.0  0.0    796   492 ?        Ss   04:38   0:00 /pause
root         551  2.7  0.5 11740152 90540 ?      Ssl  04:38  15:26 etcd --advertise-client-urls=https://172.26.0.3:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --feature-gates=InitialCorruptCheck=true --initial-advertise-peer-urls=https://172.26.0.3:2380 --initial-cluster=my-cluster-control-plane=https://172.26.0.3:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://172.26.0.3:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://172.26.0.3:2380 --name=my-cluster-control-plane --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --watch-progress-notify-interval=5s
root         558  5.0  1.8 1512492 308652 ?      Ssl  04:38  28:55 kube-apiserver --advertise-address=172.26.0.3 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --runtime-config= --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/16 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root         560  0.9  0.3 1275532 65280 ?       Ssl  04:38   5:31 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authorization-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true
root         567  2.0  0.7 1297008 116740 ?      Ssl  04:38  11:32 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --cluster-name=my-cluster --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokencleaner --enable-hostpath-provisioner=true --kubeconfig=/etc/kubernetes/controller-manager.conf --leader-elect=true --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --root-ca-file=/etc/kubernetes/pki/ca.crt --service-account-private-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/16 --use-service-account-credentials=true
root         807  0.0  0.0 1233456 14592 ?       Sl   04:38   0:07 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c52374d0678021d0c57d5b0d091ebb333f8dedf70869e2dd2c532edf24229583 -address /run/containerd/containerd.sock
root         839  0.0  0.0 1233456 12616 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 2aedf52822286efeee8cbc439930287c0332865b2215a9a943017dbbb1f71c6d -address /run/containerd/containerd.sock
root         851  0.0  0.0 1233456 14676 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 9660c93a9f9f039c20b5087d67ca20527c5c1547bf2c2899403012d57be17a31 -address /run/containerd/containerd.sock
65535        932  0.0  0.0    796   496 ?        Ss   04:38   0:00 /pause
root         947  0.0  0.0 1233456 14652 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id fecc64bdf6ccfbaad63427340a47b755dc0ca6a1d76e0e24c6f46c31126639db -address /run/containerd/containerd.sock
65535        958  0.0  0.0    796   492 ?        Ss   04:38   0:00 /pause
65535        960  0.0  0.0    796   496 ?        Ss   04:38   0:00 /pause
root         999  0.0  0.0 1233456 14540 ?       Sl   04:38   0:06 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id c7906e49685cd5e046956390b0cb979aa78bac9cbc1def531c340daa050e0381 -address /run/containerd/containerd.sock
65535       1001  0.0  0.0    796   492 ?        Ss   04:38   0:00 /pause
65535       1050  0.0  0.0    796   492 ?        Ss   04:38   0:00 /pause
root        1091  0.0  0.3 1270860 52856 ?       Ssl  04:38   0:10 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=my-cluster-control-plane
65532       1098  0.1  0.3 1297352 63808 ?       Ssl  04:38   1:02 /coredns -conf /etc/coredns/Corefile
65532       1134  0.1  0.3 1297096 64496 ?       Ssl  04:38   1:03 /coredns -conf /etc/coredns/Corefile
root        1138  0.0  0.2 1279192 40492 ?       Ssl  04:38   0:15 /bin/kindnetd
root        1464  0.0  0.2 1266292 36388 ?       Ssl  04:39   0:06 local-path-provisioner --debug start --helper-image docker.io/kindest/local-path-helper:v20241212-8ac705d0 --config /etc/config/config.json
root        4267  0.0  0.0   2324  1512 pts/0    Ss+  08:41   0:00 sh
root        7912  0.0  0.0   4060  3260 pts/1    Ss   14:07   0:00 bash
root        7919  0.0  0.0   8020  3868 pts/1    R+   14:07   0:00 ps -auxww
#
