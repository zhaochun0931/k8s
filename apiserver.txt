At the highest level, Kubernetes is an orchestrator of containerized apps. Ideally microservice apps. Microservice app is just a fancy name for an application that’s made up of lots of small and independent parts - we sometimes call these small parts services. 
These small independent services work together to create a meaningful/useful app.


In the real world, a football (soccer) team is made up of individuals. No two are the same, and each has a different role to play in the team. 
Some defend, some attack, some are great at passing, some are great at shooting…. Along comes the coach, and he or she gives everyone a position and organizes them into a team with a plan.



At the time of writing, the best way to package and deploy a Kubernetes application is via something called a Deployment. 
With Deployments, we start out with our application code and we containerize it. Then we define it as a Deployment via a YAML or JSON manifest file. This manifest file tells Kubernetes two important things:
What our app should look like – what images to use, ports to expose, networks to join, how to perform update etc.
How many replicas of each part of the app to run (scale)


master:

1. kube-apiserver
2. etcd database
3. kube-controller-manager
4. kube-scheduler
5. cloud-controller-manager






kube-apiserver

The API Server (apiserver) is the frontend into the Kubernetes control plane. You can think of the API server as the brains of the cluster. By default, it exposes a RESTful endpoint on port 443

kube-apiserver 支持同时提供 https（默认监听在 6443 端口）和 http API（默认监听在 127.0.0.1 的 8080 端口），其中 http API 是非安全接口，不做任何认证授权机制，不建议生产环境启用。两个接口提供的 REST API 格式相同


在实际使用中，通常通过 kubectl 来访问 apiserver，也可以通过 Kubernetes 各个语言的 client 库来访问 apiserver。在使用 kubectl 时，打开调试日志也可以看到每个 API 调用的格式，比如
kubectl --v=8 get pods










The cluster store
If the API Server is the brains of the cluster, the cluster store is its memory, The cluster store is based on etcd, the popular distributed, consistent and watchable key-value store.


kube-controller-manager
They tend to sit in loops and watch for changes – the aim of the game is to make sure the current state of the cluster matches the desired state (more on this shortly).











cloud-controller-manager







node:

1. kubelet
2. kube-proxy
3. Container runtime



每台机器上都运行一个 kube-proxy 服务，它监听 API server 中 service 和 endpoint 的变化情况，并通过 iptables 等来为服务配置负载均衡（仅支持 TCP 和 UDP）.
kube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行.













每个Node节点上都运行一个 Kubelet 服务进程，默认监听 10250 端口，接收并执行 Master 发来的指令，管理 Pod 及 Pod 中的容器。每个 Kubelet 进程会在 API Server 上注册所在Node节点的信息，定期向 Master 节点汇报该节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源.

Kubelet 监听 etcd，所有针对 Pod 的操作都将会被 Kubelet 监听到。如果发现有新的绑定到本节点的 Pod，则按照 Pod 清单的要求创建该 Pod。

如果发现本地的 Pod 被修改，则 Kubelet 会做出相应的修改，比如删除 Pod 中某个容器时，则通过 Docker Client 删除该容器。 如果发现删除本节点的 Pod，则删除相应的 Pod，并通过 Docker Client 删除 Pod 中的容器。

Kubelet 读取监听到的信息，如果是创建和修改 Pod 任务，则执行如下处理：

为该 Pod 创建一个数据目录；
从 API Server 读取该 Pod 清单；
为该 Pod 挂载外部卷；
下载 Pod 用到的 Secret；
检查已经在节点上运行的 Pod，如果该 Pod 没有容器或 Pause 容器没有启动，则先停止 Pod 里所有容器的进程。如果在 Pod 中有需要删除的容器，则删除这些容器；
用 “kubernetes/pause” 镜像为每个 Pod 创建一个容器。Pause 容器用于接管 Pod 中所有其他容器的网络。每创建一个新的 Pod，Kubelet 都会先创建一个 Pause 容器，然后创建其他容器。
为 Pod 中的每个容器做如下处理：
为容器计算一个 hash 值，然后用容器的名字去 Docker 查询对应容器的 hash 值。若查找到容器，且两者 hash 值不同，则停止 Docker 中容器的进程，并停止与之关联的 Pause 容器的进程；若两者相同，则不做任何处理；
如果容器被终止了，且容器没有指定的 restartPolicy，则不做任何处理；
调用 Docker Client 下载容器镜像，调用 Docker Client 运行容器。





In Kubernetes, the two concepts work like this:
1. We declare the desired state of our application (microservice) in a manifest file
2. We POST it the API server
3. Kubernetes stores this in the cluster store as the application’s desired state
4. Kubernetes deploys the application on the cluster
5. Kubernetes implements watch loops to make sure the cluster doesn’t vary from desired state
















Port 6443 is used by default for the Kubernetes API server's HTTPS service


kubectl cluster-info





# kubectl cluster-info
Kubernetes control plane is running at https://10.1.0.4:6443
CoreDNS is running at https://10.1.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
#







# kubectl get nodes
NAME   STATUS   ROLES           AGE     VERSION
gf1    Ready    control-plane   3d15h   v1.32.3
#

cat ~/.kube/config | egrep -i 'certificate-authority-data' | awk -F: '{print $2}' | tr -d ' ' | base64 -d > myca.pem

cat ~/.kube/config | egrep -i 'client-certificate-data' | awk -F: '{print $2}' | tr -d ' ' | base64 -d > mycert.pem

cat ~/.kube/config | egrep -i 'client-key-data' | awk -F: '{print $2}' | tr -d ' ' | base64 -d > mykey.pem

curl --cacert myca.pem --cert mycert.pem --key mykey.pem https://gf1:6443
