a Deployment cannot run two different Pods.

A Deployment = one Pod template = one type of Pod replicated many times.

So:

❌ Two different Pods in one Deployment
✔ Multiple containers inside one Pod
✔ Multiple Deployments, each with its own Pod type
✔ One Deployment = one ReplicaSet = one Pod spec

✅ Why?

A Deployment creates a ReplicaSet, and a ReplicaSet requires all Pods to be identical.
Kubernetes does not allow a Deployment to have two different Pod specs.








✅ Containers in the same Pod share the network namespace

They share IP, ports, and localhost.

But Kubernetes DNS only resolves Service names, not container names inside a Pod.

So ping nginx does not work unless there is a Service named nginx.




✅ Pod IPs in a cluster

Kubernetes assigns each Pod an IP from the Pod network.

All Pods in the same cluster usually share a single cluster-wide subnet, e.g., 10.244.0.0/16 if you use Flannel, or 192.168.0.0/16 for Calico default.














pod
|
|
|
replicaset
|
|
|
deployment






Deployments manage ReplicaSets, and ReplicaSets manage Pods.



More often than not, you’re going to deploy your applications via Deployments rather than ReplicaSets. However, Deployments build on top of ReplicaSets.

One ReplicaSet can only manage one Pod type.




A ReplicaSet defines two important things:
The Pod template
The desired number of replicas




Desired state is what you want. Current state is what you’ve got. The aim-ofthe-game is for the two to match - current state should always match desired state.

A declarative model lets you describe your desired state without having to get into the detail of how to get there.

We’ve got two competing models. The declarative model and the imperative model.
Declarative models are all about describing the end-goal. Imperative models are all about lists of commands that will get you to an end goal.




Reconciliation loops

Fundamental to all of this, are background reconciliation loops.

ReplicaSets implement a background reconciliation loop that is constantly monitoring the cluster. It’s checking to see if current state matches desired state.
If it doesn’t, it wakes up the control-plane and Kubernetes takes steps to fix the situation.

To be crystal clear - Kubernetes is constantly striving for harmony between current state and desired state!




kubectl expose deployment helloapp1 --target-port=8080 --type=NodePort
